---
title: "Variable selection"
output: word_document
---




```{r}
#### Loading the data 
STATCAN_data<-read.table("statcan.txt") # make sure statcan.txt is in your working directory
m <- ncol(STATCAN_data)
STATCAN_data <- STATCAN_data[,2:m] #The first column contains the dates, so it's not important 
STATCAN_names <-c("Y1_HOURS","X1_HOURSMAN","X2_HOURSWSRT","Y2_CPI","Y3_EMP","Y4_USCAN","Y5_ROWCAN","Y6_CANUS","Y7_CANROW","Y8_RAIL","Y9_DOM","Y10_TRAN","Y11_INT","Y12_MSM","Y13_MRTS","Y14_IMP","Y15_EXP","Y16_GDP")
colnames(STATCAN_data) <- STATCAN_names
str(STATCAN_data) # check the type of object - at this moment this is 'data.frame', we need 'Time Series'
```

```{r}
par(mfrow = c(2,2))
plot.ts(STATCAN_data$Y3_EMP)
plot.ts(STATCAN_data$Y9_DOM)
plot.ts(STATCAN_data$Y10_TRAN)
plot.ts(STATCAN_data$Y11_INT)
```




```{r}
#### Removing the time series with strong seasonality
toremove=c(which(colnames(STATCAN_data) == "Y3_EMP"),which(colnames(STATCAN_data) == "Y9_DOM"),which(colnames(STATCAN_data) == "Y10_TRAN"),which(colnames(STATCAN_data) == "Y11_INT"))
STATCAN_data <- subset(STATCAN_data, select=-toremove)
```


```{r}
#Blocks argument for the ML approach in the nowcasting package
Blocks <- matrix(1,ncol(STATCAN_data), 1)
```


```{r}
#### Converting the dataframe object to a time series object (monthly data)
STATCAN_ts<-ts(STATCAN_data,frequency=12, class = c("ts"))
```


```{r}
#### Making the time series stationary


#Now the time series will be transformed to make them stationary. To achieve that, I'll difference them once (i.e Yt will be replaced by Yt-Yt-1).
#For this I use the Bpanel(base,trans,....) function (in the nowcasting package). 
#It's first argument "base" contains the time series to transform. 
#The second argument indicates "trans" is a vector whose ith element indicates the transformation to do on the ith time series of "base". if this ith element is 2, then the ith time series will be differenced once.

library(nowcasting)
Transformations <- as.integer(rep(1,ncol(STATCAN_ts))) #vector of transformations for the Bpanel() function. the inner function returns something of type "num" and as.integer() converts it to something of type "int".
STATCAN_ts.stat <- Bpanel(base = STATCAN_ts, trans = Transformations) #There are 12 NAs at the end of each time series. Those are used in the nowcasting function for the out of sample forecasts. The function will not work without them.
```



```{r}
#Removing the means for each time series
seriesmeans <- as.vector(colMeans(STATCAN_ts.stat[1:181,]))
meansmatrix <- matrix(rep(seriesmeans,nrow(STATCAN_ts.stat)),nrow = nrow(STATCAN_ts.stat),byrow = TRUE)
STATCAN_ts.stat <- STATCAN_ts.stat - meansmatrix
```





```{r}
par(mfrow = c(2,2))
plot.ts(STATCAN_ts.stat[,3])
lines(rep(0,nrow(STATCAN_ts.stat)),col = "red")
plot.ts(STATCAN_ts.stat[,6])
lines(rep(0,nrow(STATCAN_ts.stat)),col = "red")
plot.ts(STATCAN_ts.stat[,10])
lines(rep(0,nrow(STATCAN_ts.stat)),col = "red")
plot.ts(STATCAN_ts.stat[,14])
lines(rep(0,nrow(STATCAN_ts.stat)),col = "red")
```



```{r}
#### Storing the dependent variable (GDP) and the independent variables


gdp_position_CANGDP <- which(colnames(STATCAN_ts.stat) == "Y16_GDP") # which column contains GDP;

x_CANGDP <- STATCAN_ts.stat[,-gdp_position_CANGDP]
y_CANGDP <- STATCAN_ts.stat[,gdp_position_CANGDP]

data_CANGDP <- cbind(y_CANGDP, x_CANGDP) #I put the GDP as the first column. This formating is necessary for the nowcasting function to work. 

#Removing the NAs for the PCA regression part
x_CANGDP <-na.omit(x_CANGDP, na.action = "exclude" ) 
y_CANGDP <- na.omit(y_CANGDP, na.action = "exclude")


#data_CANGDP
```


```{r}
## libraries for data visualization
library(ggplot2)
library(gridExtra)

library(vars) #package for VAR
```


```{r}
#Installing the dynfactoR package
devtools::install_github("rbagd/dynfactoR")
```





```{r}
predictor_names <- c("Y1_HOURS","X1_HOURSMAN","X2_HOURSWSRT","Y2_CPI","Y4_USCAN","Y5_ROWCAN","Y6_CANUS","Y7_CANROW","Y8_RAIL","Y12_MSM","Y13_MRTS","Y14_IMP","Y15_EXP")

predictor_colours <- c("Y1_HOURS"="blue","X1_HOURSMAN"="red", "X2_HOURSWSRT"="black", "Y2_CPI"="orange", "Y4_USCAN"="gray","Y5_ROWCAN"="yellow","Y6_CANUS"= "green", "Y7_CANROW"="#E7298A", "Y8_RAIL"="#A6761D","Y12_MSM"= "#7570B3","Y13_MRTS"="aquamarine3","Y14_IMP"="antiquewhite4","Y15_EXP"="darkmagenta" )

R_2s <- rep(0,length(predictor_names))

for(i in 1:length(R_2s))
{
   R_2 <- summary(lm(y_CANGDP~x_CANGDP[,predictor_names[i]]))$r.squared
   R_2s[i] <- R_2
}

R_2s_data <- data.frame(R_2s,predictor_names)
#R_2s_data <- R_2s_data[order(R_2s_data$R_2s,decreasing = TRUE),]

ggplot(R_2s_data,aes(x= reorder(predictor_names,R_2s),R_2s,fill = predictor_names))+geom_bar(stat ="identity")+coord_flip()+ scale_fill_manual("predictor_names",  values=predictor_colours) +  xlab("Predictor") + ylab("R^2") #+ggtitle("R^2 for each predictor")

#barplot(R_2s_data$R_2s, main="R^2, Y_t ~ X_it", horiz=FALSE,
  #names.arg=R_2s_data$predictor_names,las = 2)
```

Let's replace the variables Y7_CANROW, Y2_CPI, Y6_CANUS, X1_HOURSMAN by values I generate randomly, independently of the dependent variable.

```{r}

```



```{r}
Factor_names <- c("Factor 1","Factor 2","Factor 3","Factor 4","Factor 5","Factor 6","Factor 7","Factor 8")#,"Factor 9","Factor 10","Factor 11","Factor 12")
Factors <- dynfactoR::dfm(x_CANGDP,r=8,p=3)$twostep
plotslist <- vector(mode = "list", 8)
for(j in 1:8)
{
   R_2sfac <- rep(0,length(predictor_names))

for(i in 1:length(R_2s))
{
   R_2fac <- summary(lm(Factors[,j]~x_CANGDP[,predictor_names[i]]))$r.squared
   R_2sfac[i] <- R_2fac
}

R_2sfac_data <- data.frame(R_2sfac,predictor_names)


plotslist[[j]]<- ggplot(R_2sfac_data,aes(x= reorder(predictor_names,R_2sfac),R_2sfac,fill = predictor_names))+geom_bar(stat ="identity")+coord_flip()+ scale_fill_manual("predictor_names",  values=predictor_colours) +  xlab("Predictor") + ylab("R^2") +ggtitle(Factor_names[j])
   
}
```


```{r}
#grid.arrange(plotslist[[1]],plotslist[[2]],nrow = 2)
#grid.arrange(plotslist[[3]],plotslist[[4]],nrow = 2)
#grid.arrange(plotslist[[5]],plotslist[[6]],nrow = 2)
#grid.arrange(plotslist[[7]],plotslist[[8]],nrow = 2)
#grid.arrange(plotslist[[9]],plotslist[[10]],nrow = 2)
#grid.arrange(plotslist[[11]],plotslist[[12]],nrow = 2)
plotslist[[1]]; plotslist[[2]]; plotslist[[3]] ; plotslist[[4]];  plotslist[[5]] ; plotslist[[6]];  plotslist[[7]]; plotslist[[8]]#; plotslist[[9]]; plotslist[[10]]; plotslist[[11]];  plotslist[[12]]

```





































Simulation for indirect approach

```{r}
CANGDP_feat_reg<-lm(y_CANGDP~x_CANGDP)
CANGDP_feat_reg_coeff<- CANGDP_feat_reg$coefficients
a_1 <- CANGDP_feat_reg_coeff[1];a_2 <- CANGDP_feat_reg_coeff[2];a_3 <- CANGDP_feat_reg_coeff[3];a_4<- CANGDP_feat_reg_coeff[4];a_5 <- CANGDP_feat_reg_coeff[5];a_6 <- CANGDP_feat_reg_coeff[6];a_7 <- CANGDP_feat_reg_coeff[7];a_8 <- CANGDP_feat_reg_coeff[8];a_9 <- CANGDP_feat_reg_coeff[9];a_10 <- CANGDP_feat_reg_coeff[10]
```



```{r}
TS_1 <- as.ts(rnorm(180))
TS_2 <- as.ts(rnorm(180))
TS_3 <- as.ts(rnorm(180))
TS_4 <- as.ts(rnorm(180))
TS_5 <- as.ts(rnorm(180))
TS_6 <- as.ts(rnorm(180, sd = 3))
TS_7 <- as.ts(rnorm(180,sd = 4))
TS_8 <- as.ts(rnorm(180, sd =5))
TS_9 <- as.ts(rnorm(180, sd = 6))
TS_10 <- as.ts(rnorm(180, sd = 7))
         
#### generate error term
normerror=rnorm(length(TS_1))
         
#### Generating the dependent variable
Y = a_1*TS_1 + a_2*TS_2 + a_3*TS_3 + a_4*TS_4 + a_5*TS_5 + a_6*TS_6 + a_7*TS_7 + a_8*TS_8 + a_9*TS_9 +a_10*TS_10 + normerror
        
   
TS = matrix(c(TS_1, TS_2, TS_3, TS_4, TS_5, TS_6, TS_7, TS_8, TS_9,TS_10), ncol = 10)

DFMs <- vector(mode = "list", length = 12)
          tryCatch( { 
          
             for (k in 1:12){
                #print(k)
             
                DFMs[[k]] <- dynfactoR::dfm(TS,r=k,p=1)
                
                }
             }, error = function(e) {DFMerror <<- TRUE})


```








Simulation for direct approach

```{r}

```


```{r}
TS_1 <- as.ts(rnorm(180))
TS_2 <- as.ts(rnorm(180))
TS_3 <- as.ts(rnorm(180))
TS_4 <- as.ts(rnorm(180))
TS_5 <- as.ts(rnorm(180))
TS_6 <- as.ts(rnorm(180, sd = 3))
TS_7 <- as.ts(rnorm(180,sd = 4))
TS_8 <- as.ts(rnorm(180, sd =5))
TS_9 <- as.ts(rnorm(180, sd = 6))
TS_10 <- as.ts(rnorm(180, sd = 7))
         
         
TS = matrix(c(TS_1, TS_2, TS_3, TS_4, TS_5, TS_6, TS_7, TS_8, TS_9,TS_10), ncol = 10)
         
#### generate error term
normerror=rnorm(177)
         
#### Generating the dependent variable
Y = cbind(TS[3:179,],TS[2:178,],TS[1:177,]) %*% lag_reg_coeff  + matrix(normerror,ncol = 1)
```


















































II) Creating VAR correlation matrix

```{r}

library(Matrix)
#### generating correlation matrices

#corr1 (range: 0.1-0.3)
#generating the components of the matrix
#set.seed(6)
n = runif(36, min = 0.1, max = 0.3)
# creating the matrix
corr1 <- matrix(n,6,6)
# making it symmetric
ind <- lower.tri(corr1) 
corr1[ind] <- t(corr1)[ind] 
diag(corr1) <- 1
#corr1
#eigen(corr1)$values
corr1 <- nearPD(corr1, corr = TRUE)
corr1 <- corr1$mat
#corr1
eigen(corr1)$values


#corr2 (range: 0.3-0.6)
#generating the components of the matrix
#set.seed(6)
n = runif(36, min = 0.3, max = 0.6)
# creating the matrix
corr2 <- matrix(n,6,6)
# making it symmetric
ind <- lower.tri(corr2) 
corr2[ind] <- t(corr2)[ind] 
diag(corr2) <- 1
#corr2
#eigen(corr2)$values
corr2 <- nearPD(corr2, corr = TRUE)
corr2 <- corr2$mat
#corr1
eigen(corr2)$values


#corr3 (range: 0.6-0.9)
#generating the components of the matrix
#set.seed(6)
n = runif(36, min = 0.6, max = 0.9)
# creating the matrix
corr3 <- matrix(n,6,6)
# making it symmetric
ind <- lower.tri(corr3) 
corr3[ind] <- t(corr3)[ind] 
diag(corr3) <- 1
#corr3
#eigen(corr3)$values
corr3 <- nearPD(corr3, corr = TRUE)
corr3 <- corr3$mat
#corr3
eigen(corr3)$values



#corr4 (range: between -0.9 and 0.9)
#generating the components of the matrix
#set.seed(6)
n = runif(36, min = -0.9, max = 0.9)
# creating the matrix
corr4 <- matrix(n,6,6)
# making it symmetric
ind <- lower.tri(corr4) 
corr4[ind] <- t(corr4)[ind] 
diag(corr4) <- 1
#corr4
#eigen(corr4)$values
corr4 <- nearPD(corr4, corr = TRUE)
corr4 <- corr4$mat
#corr4
eigen(corr4)$values





sigma_list <- list(corr1, corr2, corr3, corr4)
sigma_names = c("corr1 (0.1;0.3)", "corr2 (0.3;0.6)", "corr3 (0.6;0.9)","corr4 (-0.9;0.9)")


```



III) Simulating and ploting 100 observations

```{r include=FALSE}
library(MTS)
par(mfrow=c(2,2))
for (i in 1:4)
  {
   #set.seed(i+10)

   sim <- VARMAsim(100,arlags = 1,phi=coeff,sigma=sigma_list[[i]])
   TS_1 <- as.ts(sim$series[,1])
   TS_2 <- as.ts(sim$series[,2])
   TS_3 <- as.ts(sim$series[,3])
   TS_4 <- as.ts(sim$series[,4])
   TS_5 <- as.ts(sim$series[,5])
   TS_6 <- as.ts(sim$series[,6])
   #TS_7 <- as.ts(sim$series[,7])
   #TS_8 <- as.ts(sim$series[,8])
   #TS_9 <- as.ts(sim$series[,9])
   #TS_10 <- as.ts(sim$series[,10])
   
   
   ts.plot(TS_1, TS_2, TS_3, TS_4, TS_5, col=c("blue","red", "black", "orange", "gray", "yellow"), main=sigma_names[i])
}



```

IV) Analysis with normal error

```{r}
a_1 = 0.6
a_2 = 4
a_3 = 0.2
a_4 = 0.9
a_5 = 2
a_6 = 3
#a_7 = 0.5
#a_8 = 0.7
#a_9 = 0.9
#a_10 = 0.4

par(mfrow = c(2,2))

#A multidimensional list to store the results of the nowcasting. there are 6 standard deviations for the error term. For each standard deviation there are 4 levels of correlation matrices for the simulated time series.
nowcast_results <- vector(mode = "list", length = 6) 
for (i in 1:6)
{
   nowcast_results[[i]] = vector(mode = "list", length = 4)
   #each element of this list has three compnents: 1) the standardized independent variables , 2) the factors, 3) the eigenvectors
}



sds = c(2.5,3,4,6,10,20) # a set of standard deviations for error terms
sds_names = c('sd = 2.5','sd = 3', 'sd = 4', 'sd = 6', 'sd = 10', 'sd = 20')




#nowcasting library
library(nowcasting)

for (j in 1:6)  #iterates over each standard deviation
   {
   #print(sds_names[j])
   options(error = traceback) #to catch eventual errors

   normerror=rnorm(length(TS_1), sd = sds[j]) #generates a random vector from a normal distribution





for (i in 1:4)  #iterates over each correlation matrix
{
  par(mfrow=c(2,2))
  
   sim <- VARMAsim(100,arlags = 1,phi=coeff,sigma=as.matrix(sigma_list[[i]]))
   TS_1 <- as.ts(sim$series[,1])
   TS_2 <- as.ts(sim$series[,2])
   TS_3 <- as.ts(sim$series[,3])
   TS_4 <- as.ts(sim$series[,4])
   TS_5 <- as.ts(sim$series[,5])
   TS_6 <- as.ts(sim$series[,6])
   
   #generating TS_7, TS_8, TS_9 and TS_10 independently of TS_1,...,TS_6,Y. TS_9 and TS_10 have higher variance than TS_1,...,TS_8
   TS_7 <- rnorm(n = 100, mean = 0, sd = 1)
   TS_8 <- rnorm(n = 100, mean = 0, sd = 1)
   TS_9 <- rnorm(n = 100, mean = 0, sd = sqrt(7))
   TS_10 <- rnorm(n = 100, mean = 0, sd = sqrt(8))
   
   Y = a_1*TS_1 + a_2*TS_2 + a_3*TS_3 + a_4*TS_4 + a_5*TS_5 + a_6*TS_6 + normerror
   #print(sd(Y))
   
   
   
   TS = matrix(c(TS_1, TS_2, TS_3, TS_4, TS_5, TS_6, TS_7, TS_8, TS_9,TS_10), ncol = 10)
  

   
   
   ###################################################################
   #### Nowcasting
   
   
   SIM_ts<-ts(TS,deltat = 1/12, class="mts") # Convert to a "Time Series" object; monthly data
  
   Transformations <- as.integer(rep(0,ncol(SIM_ts))) #Creates a vector of zeros of size 100. But for what?Will be used in the Bpanel function later
   Legend <- data.frame(Transformations) #transform the vector of zeros into a data frame object. Why the need to transform it to a datframe object?
   SIM <- list(data = SIM_ts,legend = Legend) #Creates a list object from SIM_ts and Legend?What is the structure of the list? 
   base.data_SIM <- SIM$data #what does it do?
   stat.tr_SIM <- SIM$legend$Transformations# What does it do ?
   x_SIM <- Bpanel(base = base.data_SIM, trans = stat.tr_SIM)  #TS[,1] should be the same as x_SIM[,1] since no transformation occured. Didn't do any transformation since the series already seem stationary.
   #The Bpanel() function transforms the original monthly time series to its stationary representation following the user specification. The time series with more than 1/3 missing, i.e.     NAs, are deleted    and the remaining are modified such that the missings and outliers are replaced by an approximated value.
   # the trans argument is A vector where each coordinate is a code for   the transformation of the correspondent coordinate in the base argument. The transformation is specified by         codes. trans = 0: the  original serie is preserved
   
   #the NULLs come from the above statement (Bpnalel function)
   
   y_SIM<-ts(Y,frequency=12,class=c("ts")) #Convert the simulated time series Y to a "time series" object, monthly data
  

   data_SIM <- cbind(y_SIM, x_SIM) #Combines y_SIM and x_SIM into a  single time series object
  
   frequency_SIM <- c(12, rep(12, ncol(data_SIM)-1)) #creates a vector of two objects: the number 12 and  a vector containing 12 100 times.For what purpose?used in the nowcast() function

   
      
   #Performs the nowcasting for 9 factors
   now2s <- nowcast(formula = y_SIM ~ ., data = data_SIM, r = 9, p   = 1, q = 9, method = '2s', frequency = frequency_SIM)
   
   
   #### Storing the transformed independent variables, the factors and the eigenvalues
   
   results <- vector(mode = "list", length = 3) 
   
   #transforming the regressors to have zero mean and unit variance
   trans_x  <- matrix(data = NA, nrow = nrow(now2s$xfcst), ncol = ncol(now2s$xfcst))
   for(k in 1:ncol(trans_x))
      {
      trans_x[,k] = (now2s$xfcst[,k]- mean(now2s$xfcst[,k]))*(1/sd(now2s$xfcst[,k]))
      }
   results[[1]] <- trans_x
   
   #Storing the transformed independent variables
   results[[2]] <- now2s$factors$dynamic_factors
   
   #Storing the eigenvalues
   results[[3]] <- now2s$factors$eigen$values
   
   #Storing the factors
   nowcast_results[[j]][[i]] <- results
   

  ##############################################################################
  #### Plotting the eigenvalues (The ith eigenvalue corresponds to the contribution of the ith principal component to the variability in the dataset).
   
  #nowcast.plot(now2s, type = "eigenvalues")
}






}

#nowcast_results

```


V) Contributions of the variables to the model

```{r}
#### Checking that the factors are orthogonal 'enough'. All values are very close to 0, so we can assume orthogonality
library(lsa)
#for(i in 1:6)
   #{
   #for(j in 1:4)
      #{
      #factors <-  nowcast_results[[i]][[j]][[2]]
      #for(k in 1:9)
         #{
         #for(l in 1:9)
            #{
            #if(l!=k)
               #{
               #print(cosine(as.vector( factors[,k]),as.vector(factors[,l])))
               #}
            #}
         #}
     #}
#}

```



```{r}

#### Contributions of the variables to the model (using cosine for variable to factor contribution)


#For each level of error standard deviation, I have 4 plots, each corresponding to a correlation matrix.
par(mfrow = c(2,2))
for (i in 1:6) #for each level of standard deviation of the error terms
{
   print(sds_names[i])
   for(j in 1:4) #For each correlation matrix
   {
      options(error = traceback)
   
      w = rep(0,10) #a vector containing to the contribution of each feature Xi to the model
      
      factors = nowcast_results[[i]][[j]][[2]]
      #print(dim(factors))
      x_trans = nowcast_results[[i]][[j]][[1]]
      
      
      #### contribution of each factor to the variability in the dataset
      factors_contrib = rep(0,9)
      for(k in 1:9) #For each factor Fk
         {
         factors_contrib[k]  = var(factors[,k]) #the factor contribution is computed as its sample variance.
         }
      factors_contrib = (1/sum(factors_contrib))*factors_contrib #putting the contribution of each factor in percentage




      #### contribution of TS_k (Xk) to Fl
      feat_contrib = matrix(data = NA, nrow = 10, ncol = 9) #10 independent variables and 9 factors.
      for(k in 1:10) #For each variable TS_k (Xk)
         {
         for(l in 1:9) #For each factor Fl
            {
            Xk = as.vector(x_trans[,k])
            Fl = as.vector(factors[,l])
            feat_contrib[k,l] = abs(cosine(Xk,Fl)) #the contribution of Xk to Fl is computed as the absolute value of the cosine similarity of the two vectors.
            }
         }
 

      for(k in 1:ncol(feat_contrib)) #for each factor Fk
         { 
         feat_contrib[,k] =(1/sum(feat_contrib[,k])) *feat_contrib[,k] #putting the contribution of each variable in percentage
         }

      for(k in 1:10) #For each feature Xk
         {
         #plot(c(1:23),(eigenvectors[,i]^2)*100, type = "h")
         w[k] = sum(feat_contrib[k,]*factors_contrib)*100
         }

plot(c(1:10),w, type = "h", main = sigma_names[j],xlab = "Features", ylab = "Contribution (in %)")
axis(4,at = c(0:6) )
lines(c(0:11),rep(mean(w),12),type="l", col="red")
#print(sum(w))
   
   



  }

}
```


```{r}
#### Contributions of the variables to the model (using the eigenvectors of the sample covariance matrix of the variables)


#For each level of error standard deviation, I have 4 plots, each corresponding to a correlation matrix.
par(mfrow = c(2,2))
for (i in 1:6) #for each level of standard deviation of the error terms
{
   print(sds_names[i])
   for(j in 1:4) #for each correlation matrix
   {
      options(error = traceback)
   
      w = rep(0,10) #a vector containing to the contribution of each feature Xi to the model
      
      factors = nowcast_results[[i]][[j]][[2]]
      #print(dim(factors))
      x_trans = nowcast_results[[i]][[j]][[1]]
      
      
      #### contribution of each factor to the variability in the dataset
      factors_contrib = rep(0,9)
      for(k in 1:9) #For each factor Fk
         {
         factors_contrib[k]  = var(factors[,k]) #the factor contribution is computed as its sample variance.
         }
      factors_contrib = (1/sum(factors_contrib))*factors_contrib #putting the contribution of each factor in percentage




      #### contribution of TS_k (Xk) to Fl
      feat_contrib = matrix(data = NA, nrow = 10, ncol = 9) #10 independent variables and 9 factors.
      for(k in 1:10) #For each variable TS_k (Xk)
         {
         for(l in 1:9) #For each factor Fl
            {
           
            feat_contrib[k,l] = abs(eigen(cov(x_trans))$vectors[k,l]) #the contribution of TS_k to Fl is computed as the ith element of the lth eigen vector of the covariance matrix of the TS_k's.
            }
         }
 

      for(k in 1:ncol(feat_contrib)) #for each factor Fk
         { 
         feat_contrib[,k] =(1/sum(feat_contrib[,k])) *feat_contrib[,k] #putting the contribution of each variable in percentage
         }

      for(k in 1:10) #For each feature Xk
         {
         #plot(c(1:23),(eigenvectors[,i]^2)*100, type = "h")
         w[k] = sum(feat_contrib[k,]*factors_contrib)*100
         }

plot(c(1:10),w, type = "h", main = sigma_names[j] ,xlab = "Features", ylab = "Contribution (in %)")
axis(4,at = c(0:6) )
lines(c(0:11),rep(mean(w),12),type="l", col="red")
#print(sum(w))
   
   



  }

}
```


```{r}
#### Plotting the eigenvalues (The ith eigenvalue corresponds to the contribution of the ith principal component to the variability in the dataset).
#For each level of error standard deviation, I have 4 plots, each corresponding to a correlation matrix.
par(mfrow = c(2,2))
for (i in 1:6) #for each level of standard deviation of the error terms
{
   print(sds_names[i])
   for(j in 1:4) #for each correlation matrix
   {
      eigenvalues = nowcast_results[[i]][[j]][[3]]
      
      #percentage of each eigenvalue
      eigenvalues_percent = (1/sum(eigenvalues))*eigenvalues*100
      #print(eigenvalues_percent)
      
      
      #plotting the percentages
      plot(c(1:10),eigenvalues_percent, type = "h", main = sigma_names[j] ,xlab = "Principal components", ylab = "Contribution (in %)", col = "red")
      #axis(4,at = c(0:6) )
      
   }
   
}

```

```{r}
now2s$yfcst
```



